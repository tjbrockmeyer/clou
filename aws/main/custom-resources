#!/bin/bash

# Uploads the lambda function for all custom resources in the lambda/custom-resource directory
# Then deploys the cloud formation template that is pointing to those uploads.

DIR="$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
PACKAGE_CODE="$DIR/../scripts/package-code"

# ====
# This is code to upload all of the templates in the templates folder to S3
# But right now, that's not really useful, so it's commented...
# ====
# cd "$DIR/.."
# BUCKET=$(aws ssm get-parameter --name ' /global/bucket-name' --output text --query 'Parameter.Value')
# for FILE in templates/*.yml; do
#     aws s3api put-object --bucket "$BUCKET" --key "$FILE" --body "$FILE" >/dev/null && printf -- "finished uploading $FILE\n" &
# done

CURRENT_VERSION="$(aws cloudformation describe-stacks --stack-name custom-resources --query 'Stacks[0].Parameters[?ParameterKey==`CodeVersion`].ParameterValue' 2>/dev/null | jq -r '.[0]')"
if [[ $CURRENT_VERSION = v0 ]]; then
    VERSION=1
else
    VERSION=0
fi

cd "$DIR/../lambda/custom-resource"
BUCKET=$(aws ssm get-parameter --name ' /global/bucket-name' --output text --query 'Parameter.Value')
PIDS=
for PACKAGE in *; do 
    if [[ -e "$PACKAGE/requirements.txt" ]]; then
        "$PACKAGE_CODE" "$PACKAGE" "$BUCKET" "$PACKAGE/v$VERSION" \
            "mkdir site-packages 2>/dev/null" \
            "python -m pip install -q -r requirements.txt --target site-packages" &
        PIDS+=" $!"
    elif [[ -e "$PACKAGE/package.json" ]]; then
        "$PACKAGE_CODE" "$PACKAGE" "$BUCKET" "$PACKAGE/v$VERSION" \
            "" "NODE_ENV=production npm ci --silent" &
        PIDS+=" $!"
    else 
        echo "update-custom-resources: no valid packaging command for this directory: $PACKAGE"
    fi
done

for PID in $PIDS; do
    if ! wait $PID; then
        wait
        echo "at least one build has failed - aborting..." > /dev/stderr
        exit 1
    fi
done

cd "$DIR/.."
./bin/cfn-deploy main/custom-resources.yml custom-resources "CodeVersion=v$VERSION"